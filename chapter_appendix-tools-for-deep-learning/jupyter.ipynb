{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JDhal04/pyspark/blob/main/chapter_appendix-tools-for-deep-learning/jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31d9a4c9",
      "metadata": {
        "origin_pos": 0,
        "id": "31d9a4c9"
      },
      "source": [
        "# Using Jupyter Notebooks\n",
        ":label:`sec_jupyter`\n",
        "\n",
        "\n",
        "This section describes how to edit and run the code\n",
        "in each section of this book\n",
        "using the Jupyter Notebook. Make sure you have\n",
        "installed Jupyter and downloaded the\n",
        "code as described in\n",
        ":ref:`chap_installation`.\n",
        "If you want to know more about Jupyter see the excellent tutorial in\n",
        "their [documentation](https://jupyter.readthedocs.io/en/latest/).\n",
        "\n",
        "\n",
        "## Editing and Running the Code Locally\n",
        "\n",
        "Suppose that the local path of the book's code is `xx/yy/d2l-en/`. Use the shell to change the directory to this path (`cd xx/yy/d2l-en`) and run the command `jupyter notebook`. If your browser does not do this automatically, open http://localhost:8888 and you will see the interface of Jupyter and all the folders containing the code of the book, as shown in :numref:`fig_jupyter00`.\n",
        "\n",
        "![The folders containing the code of this book.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter00.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter00`\n",
        "\n",
        "\n",
        "You can access the notebook files by clicking on the folder displayed on the webpage.\n",
        "They usually have the suffix \".ipynb\".\n",
        "For the sake of brevity, we create a temporary \"test.ipynb\" file.\n",
        "The content displayed after you click it is\n",
        "shown in :numref:`fig_jupyter01`.\n",
        "This notebook includes a markdown cell and a code cell. The content in the markdown cell includes \"This Is a Title\" and \"This is text.\".\n",
        "The code cell contains two lines of Python code.\n",
        "\n",
        "![Markdown and code cells in the \"text.ipynb\" file.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter01.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter01`\n",
        "\n",
        "\n",
        "Double click on the markdown cell to enter edit mode.\n",
        "Add a new text string \"Hello world.\" at the end of the cell, as shown in :numref:`fig_jupyter02`.\n",
        "\n",
        "![Edit the markdown cell.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter02.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter02`\n",
        "\n",
        "\n",
        "As demonstrated in :numref:`fig_jupyter03`,\n",
        "click \"Cell\" $\\rightarrow$ \"Run Cells\" in the menu bar to run the edited cell.\n",
        "\n",
        "![Run the cell.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter03.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter03`\n",
        "\n",
        "After running, the markdown cell is shown in :numref:`fig_jupyter04`.\n",
        "\n",
        "![The markdown cell after running.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter04.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter04`\n",
        "\n",
        "\n",
        "Next, click on the code cell. Multiply the elements by 2 after the last line of code, as shown in :numref:`fig_jupyter05`.\n",
        "\n",
        "![Edit the code cell.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter05.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter05`\n",
        "\n",
        "\n",
        "You can also run the cell with a shortcut (\"Ctrl + Enter\" by default) and obtain the output result from :numref:`fig_jupyter06`.\n",
        "\n",
        "![Run the code cell to obtain the output.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter06.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter06`\n",
        "\n",
        "\n",
        "When a notebook contains more cells, we can click \"Kernel\" $\\rightarrow$ \"Restart & Run All\" in the menu bar to run all the cells in the entire notebook. By clicking \"Help\" $\\rightarrow$ \"Edit Keyboard Shortcuts\" in the menu bar, you can edit the shortcuts according to your preferences.\n",
        "\n",
        "## Advanced Options\n",
        "\n",
        "Beyond local editing two things are quite important: editing the notebooks in the markdown format and running Jupyter remotely.\n",
        "The latter matters when we want to run the code on a faster server.\n",
        "The former matters since Jupyter's native ipynb format stores a lot of auxiliary data that is\n",
        "irrelevant to the content,\n",
        "mostly related to how and where the code is run.\n",
        "This is confusing for Git, making\n",
        "reviewing contributions very difficult.\n",
        "Fortunately there is an alternative---native editing in the markdown format.\n",
        "\n",
        "### Markdown Files in Jupyter\n",
        "\n",
        "If you wish to contribute to the content of this book, you need to modify the\n",
        "source file (md file, not ipynb file) on GitHub.\n",
        "Using the notedown plugin we\n",
        "can modify notebooks in the md format directly in Jupyter.\n",
        "\n",
        "\n",
        "First, install the notedown plugin, run the Jupyter Notebook, and load the plugin:\n",
        "\n",
        "```\n",
        "pip install d2l-notedown  # You may need to uninstall the original notedown.\n",
        "jupyter notebook --NotebookApp.contents_manager_class='notedown.NotedownContentsManager'\n",
        "```\n",
        "\n",
        "You may also turn on the notedown plugin by default whenever you run the Jupyter Notebook.\n",
        "First, generate a Jupyter Notebook configuration file (if it has already been generated, you can skip this step).\n",
        "\n",
        "```\n",
        "jupyter notebook --generate-config\n",
        "```\n",
        "\n",
        "Then, add the following line to the end of the Jupyter Notebook configuration file (for Linux or macOS, usually in the path `~/.jupyter/jupyter_notebook_config.py`):\n",
        "\n",
        "```\n",
        "c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'\n",
        "```\n",
        "\n",
        "After that, you only need to run the `jupyter notebook` command to turn on the notedown plugin by default.\n",
        "\n",
        "### Running Jupyter Notebooks on a Remote Server\n",
        "\n",
        "Sometimes, you may want to run Jupyter notebooks on a remote server and access it through a browser on your local computer. If Linux or macOS is installed on your local machine (Windows can also support this function through third-party software such as PuTTY), you can use port forwarding:\n",
        "\n",
        "```\n",
        "ssh myserver -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "The above string `myserver` is the address of the remote server.\n",
        "Then we can use http://localhost:8888 to access the remote server `myserver` that runs Jupyter notebooks. We will detail on how to run Jupyter notebooks on AWS instances\n",
        "later in this appendix.\n",
        "\n",
        "### Timing\n",
        "\n",
        "We can use the `ExecuteTime` plugin to time the execution of each code cell in Jupyter notebooks.\n",
        "Use the following commands to install the plugin:\n",
        "\n",
        "```\n",
        "pip install jupyter_contrib_nbextensions\n",
        "jupyter contrib nbextension install --user\n",
        "jupyter nbextension enable execute_time/ExecuteTime\n",
        "```\n",
        "\n",
        "## Summary\n",
        "\n",
        "* Using the Jupyter Notebook tool, we can edit, run, and contribute to each section of the book.\n",
        "* We can run Jupyter notebooks on remote servers using port forwarding.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Edit and run the code in this book with the Jupyter Notebook on your local machine.\n",
        "1. Edit and run the code in this book with the Jupyter Notebook *remotely* via port forwarding.\n",
        "1. Compare the running time of the operations $\\mathbf{A}^\\top \\mathbf{B}$ and $\\mathbf{A} \\mathbf{B}$ for two square matrices in $\\mathbb{R}^{1024 \\times 1024}$. Which one is faster?\n",
        "\n",
        "\n",
        "[Discussions](https://discuss.d2l.ai/t/421)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"StudentApp\").getOrCreate()"
      ],
      "metadata": {
        "id": "3wzJx-g45v31"
      },
      "id": "3wzJx-g45v31",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"csv\").option(\"inferSchema\",\n",
        "\"true\").option(\"header\", \"true\").load(\"/content/departuredelays.csv\")"
      ],
      "metadata": {
        "id": "6i4rbJY356t1"
      },
      "id": "6i4rbJY356t1",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"flight_temp_view\")"
      ],
      "metadata": {
        "id": "jGx9r7JC8Jwc"
      },
      "id": "jGx9r7JC8Jwc",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        " SELECT distance, origin, destination\n",
        " FROM flight_temp_view\n",
        " WHERE distance > 500\n",
        " ORDER BY distance DESC\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "Tmb_Xgm89c9O",
        "outputId": "d7fb0126-4345-4171-9623-b62fba9a8360",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Tmb_Xgm89c9O",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-----------+\n",
            "|distance|origin|destination|\n",
            "+--------+------+-----------+\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "+--------+------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        " SELECT delay, origin, destination,\n",
        " CASE\n",
        " WHEN delay > 360 THEN 'Very Long Delays'\n",
        " WHEN delay > 120 AND delay <= 360 THEN 'Long Delays'\n",
        " WHEN delay > 60 AND delay <= 120 THEN 'Short Delays'\n",
        " WHEN delay > 0 AND delay <= 60 THEN 'Tolerable Delays'\n",
        " WHEN delay = 0 THEN 'No Delays'\n",
        " ELSE 'Early'\n",
        " END AS Flight_Delays\n",
        " FROM flight_temp_view\n",
        " ORDER BY origin, delay DESC\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "s7pUa0Fw9j3e",
        "outputId": "0b474e0d-97bc-4b28-d474-86fb64f7ec6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "s7pUa0Fw9j3e",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+-----------+-------------+\n",
            "|delay|origin|destination|Flight_Delays|\n",
            "+-----+------+-----------+-------------+\n",
            "|  333|   ABE|        ATL|  Long Delays|\n",
            "|  305|   ABE|        ATL|  Long Delays|\n",
            "|  275|   ABE|        ATL|  Long Delays|\n",
            "|  257|   ABE|        ATL|  Long Delays|\n",
            "|  247|   ABE|        ATL|  Long Delays|\n",
            "|  247|   ABE|        DTW|  Long Delays|\n",
            "|  219|   ABE|        ORD|  Long Delays|\n",
            "|  211|   ABE|        ATL|  Long Delays|\n",
            "|  197|   ABE|        DTW|  Long Delays|\n",
            "|  192|   ABE|        ORD|  Long Delays|\n",
            "|  180|   ABE|        ATL|  Long Delays|\n",
            "|  173|   ABE|        DTW|  Long Delays|\n",
            "|  165|   ABE|        ATL|  Long Delays|\n",
            "|  159|   ABE|        ORD|  Long Delays|\n",
            "|  159|   ABE|        ATL|  Long Delays|\n",
            "|  158|   ABE|        ATL|  Long Delays|\n",
            "|  151|   ABE|        DTW|  Long Delays|\n",
            "|  127|   ABE|        ATL|  Long Delays|\n",
            "|  121|   ABE|        DTW|  Long Delays|\n",
            "|  118|   ABE|        DTW| Short Delays|\n",
            "+-----+------+-----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, desc\n",
        "df.filter(col(\"distance\") > 500).select(\"distance\", \"origin\",\n",
        "\"destination\").orderBy(desc(\"distance\")).show()"
      ],
      "metadata": {
        "id": "dVDym4AC94b-",
        "outputId": "e1020a4e-092d-454f-b308-b1889dd85ddb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dVDym4AC94b-",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+-----------+\n",
            "|distance|origin|destination|\n",
            "+--------+------+-----------+\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "|    4330|   HNL|        JFK|\n",
            "+--------+------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create or get the Spark session with Hive support enabled\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark with Hive Support\") \\\n",
        "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "yoKlGtJT-NLu"
      },
      "id": "yoKlGtJT-NLu",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pyspark --packages org.apache.spark:spark-hive_2.12:3.1.1\n"
      ],
      "metadata": {
        "id": "7uaIY18tAZhh",
        "outputId": "7a0f98ff-e7bf-4db0-d9d4-79e58fdeab56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7uaIY18tAZhh",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0] on linux\n",
            "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
            ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-hive_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-74c8f42c-58b1-42d5-a00b-293124ec28e8;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-hive_2.12;3.1.1 in central\n",
            "\tfound org.apache.hive#hive-common;2.3.7 in central\n",
            "\tfound commons-cli#commons-cli;1.2 in central\n",
            "\tfound commons-lang#commons-lang;2.6 in central\n",
            "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
            "\tfound jline#jline;2.14.6 in central\n",
            "\tfound joda-time#joda-time;2.10.5 in central\n",
            "\tfound org.apache.commons#commons-compress;1.20 in central\n",
            "\tfound com.tdunning#json;1.8 in central\n",
            "\tfound io.dropwizard.metrics#metrics-core;4.1.1 in central\n",
            "\tfound io.dropwizard.metrics#metrics-jvm;4.1.1 in central\n",
            "\tfound io.dropwizard.metrics#metrics-json;4.1.1 in central\n",
            "\tfound com.fasterxml.jackson.core#jackson-databind;2.10.0 in central\n",
            "\tfound com.fasterxml.jackson.core#jackson-annotations;2.10.0 in central\n",
            "\tfound com.fasterxml.jackson.core#jackson-core;2.10.0 in central\n",
            "\tfound com.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 in central\n",
            "\tfound org.apache.hadoop#hadoop-common;2.7.2 in central\n",
            "\tfound org.apache.hadoop#hadoop-annotations;2.7.2 in central\n",
            "\tfound com.google.guava#guava;14.0.1 in central\n",
            "\tfound org.apache.commons#commons-math3;3.4.1 in central\n",
            "\tfound xmlenc#xmlenc;0.52 in central\n",
            "\tfound commons-httpclient#commons-httpclient;3.1 in central\n",
            "\tfound commons-logging#commons-logging;1.1.3 in central\n",
            "\tfound commons-codec#commons-codec;1.10 in central\n",
            "\tfound commons-io#commons-io;2.5 in central\n",
            "\tfound commons-net#commons-net;3.1 in central\n",
            "\tfound commons-collections#commons-collections;3.2.2 in central\n",
            "\tfound javax.servlet#servlet-api;2.5 in central\n",
            "\tfound org.mortbay.jetty#jetty;6.1.26 in central\n",
            "\tfound org.mortbay.jetty#jetty-util;6.1.26 in central\n",
            "\tfound com.sun.jersey#jersey-core;1.14 in central\n",
            "\tfound com.sun.jersey#jersey-json;1.14 in central\n",
            "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
            "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
            "\tfound javax.xml.bind#jaxb-api;2.2.2 in central\n",
            "\tfound javax.xml.stream#stax-api;1.0-2 in central\n",
            "\tfound javax.activation#activation;1.1.1 in central\n",
            "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
            "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
            "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
            "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
            "\tfound com.sun.jersey#jersey-server;1.14 in central\n",
            "\tfound asm#asm;3.2 in central\n",
            "\tfound net.java.dev.jets3t#jets3t;0.9.0 in central\n",
            "\tfound org.apache.httpcomponents#httpclient;4.5.6 in central\n",
            "\tfound org.apache.httpcomponents#httpcore;4.4.12 in central\n",
            "\tfound com.jamesmurty.utils#java-xmlbuilder;0.4 in central\n",
            "\tfound commons-configuration#commons-configuration;1.6 in central\n",
            "\tfound commons-digester#commons-digester;1.8 in central\n",
            "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
            "\tfound commons-beanutils#commons-beanutils-core;1.8.0 in central\n",
            "\tfound org.apache.avro#avro;1.8.2 in central\n",
            "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
            "\tfound org.tukaani#xz;1.5 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
            "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
            "\tfound com.google.code.gson#gson;2.2.4 in central\n",
            "\tfound org.apache.hadoop#hadoop-auth;2.7.2 in central\n",
            "\tfound org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 in central\n",
            "\tfound org.apache.directory.server#apacheds-i18n;2.0.0-M15 in central\n",
            "\tfound org.apache.directory.api#api-asn1-api;1.0.0-M20 in central\n",
            "\tfound org.apache.directory.api#api-util;1.0.0-M20 in central\n",
            "\tfound org.apache.curator#curator-framework;2.13.0 in central\n",
            "\tfound org.apache.curator#curator-client;2.13.0 in central\n",
            "\tfound com.jcraft#jsch;0.1.42 in central\n",
            "\tfound org.apache.curator#curator-recipes;2.13.0 in central\n",
            "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
            "\tfound org.apache.htrace#htrace-core;3.1.0-incubating in central\n",
            "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
            "\tfound org.apache.hive#hive-exec;2.3.7 in central\n",
            "\tfound org.apache.hive#hive-vector-code-gen;2.3.7 in central\n",
            "\tfound org.apache.velocity#velocity;1.5 in central\n",
            "\tfound oro#oro;2.0.8 in central\n",
            "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
            "\tfound org.antlr#ST4;4.0.4 in central\n",
            "\tfound org.apache.ivy#ivy;2.4.0 in central\n",
            "\tfound org.datanucleus#datanucleus-core;4.1.17 in central\n",
            "\tfound stax#stax-api;1.0.1 in central\n",
            "\tfound org.apache.hive#hive-metastore;2.3.7 in central\n",
            "\tfound javolution#javolution;5.5.1 in central\n",
            "\tfound com.jolbox#bonecp;0.8.0.RELEASE in central\n",
            "\tfound com.zaxxer#HikariCP;2.5.1 in central\n",
            "\tfound org.apache.derby#derby;10.12.1.1 in central\n",
            "\tfound org.datanucleus#datanucleus-api-jdo;4.2.4 in central\n",
            "\tfound org.datanucleus#datanucleus-rdbms;4.1.19 in central\n",
            "\tfound commons-pool#commons-pool;1.5.4 in central\n",
            "\tfound commons-dbcp#commons-dbcp;1.4 in central\n",
            "\tfound javax.jdo#jdo-api;3.0.1 in central\n",
            "\tfound javax.transaction#jta;1.1 in central\n",
            "\tfound org.datanucleus#javax.jdo;3.2.0-m3 in central\n",
            "\tfound javax.transaction#transaction-api;1.1 in central\n",
            "\tfound org.apache.hive#hive-serde;2.3.7 in central\n",
            "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
            "\tfound org.apache.hive#hive-shims;2.3.7 in central\n",
            "\tfound org.apache.hive.shims#hive-shims-common;2.3.7 in central\n",
            "\tfound org.apache.hive.shims#hive-shims-0.23;2.3.7 in central\n",
            "\tfound org.apache.hive.shims#hive-shims-scheduler;2.3.7 in central\n",
            "\tfound org.apache.hive#hive-llap-common;2.3.7 in central\n",
            "\tfound org.apache.hive#hive-llap-client;2.3.7 in central\n",
            "\tfound org.apache.avro#avro-mapred;1.8.2 in central\n",
            "\tfound org.apache.avro#avro-ipc;1.8.2 in central\n",
            "\tfound org.jodd#jodd-core;3.5.2 in central\n",
            "\tfound org.apache.thrift#libthrift;0.12.0 in central\n",
            "\tfound org.apache.thrift#libfb303;0.9.3 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-hive_2.12/3.1.1/spark-hive_2.12-3.1.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-hive_2.12;3.1.1!spark-hive_2.12.jar (60ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hive/hive-common/2.3.7/hive-common-2.3.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hive#hive-common;2.3.7!hive-common.jar (34ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hive/hive-exec/2.3.7/hive-exec-2.3.7-core.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hive#hive-exec;2.3.7!hive-exec.jar (232ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hive/hive-metastore/2.3.7/hive-metastore-2.3.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hive#hive-metastore;2.3.7!hive-metastore.jar (166ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hive/hive-serde/2.3.7/hive-serde-2.3.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hive#hive-serde;2.3.7!hive-serde.jar (39ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hive/hive-shims/2.3.7/hive-shims-2.3.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hive#hive-shims;2.3.7!hive-shims.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hive/hive-llap-common/2.3.7/hive-llap-common-2.3.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hive#hive-llap-common;2.3.7!hive-llap-common.jar (35ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hive/hive-llap-client/2.3.7/hive-llap-client-2.3.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hive#hive-llap-client;2.3.7!hive-llap-client.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.8.2/avro-1.8.2.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.avro#avro;1.8.2!avro.jar(bundle) (50ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/avro/avro-mapred/1.8.2/avro-mapred-1.8.2-hadoop2.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.avro#avro-mapred;1.8.2!avro-mapred.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar ...\n",
            "\t[SUCCESSFUL ] commons-httpclient#commons-httpclient;3.1!commons-httpclient.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.6/httpclient-4.5.6.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.6!httpclient.jar (37ms)\n",
            "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\n",
            "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (33ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.10/commons-codec-1.10.jar ...\n",
            "\t[SUCCESSFUL ] commons-codec#commons-codec;1.10!commons-codec.jar (27ms)\n",
            "downloading https://repo1.maven.org/maven2/joda-time/joda-time/2.10.5/joda-time-2.10.5.jar ...\n",
            "\t[SUCCESSFUL ] joda-time#joda-time;2.10.5!joda-time.jar (34ms)\n",
            "downloading https://repo1.maven.org/maven2/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar ...\n",
            "\t[SUCCESSFUL ] org.jodd#jodd-core;3.5.2!jodd-core.jar (31ms)\n",
            "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
            "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/org/datanucleus/datanucleus-core/4.1.17/datanucleus-core-4.1.17.jar ...\n",
            "\t[SUCCESSFUL ] org.datanucleus#datanucleus-core;4.1.17!datanucleus-core.jar (61ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/thrift/libthrift/0.12.0/libthrift-0.12.0.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.thrift#libthrift;0.12.0!libthrift.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/thrift/libfb303/0.9.3/libfb303-0.9.3.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.thrift#libfb303;0.9.3!libfb303.jar (17ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/derby/derby/10.12.1.1/derby-10.12.1.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.derby#derby;10.12.1.1!derby.jar (70ms)\n",
            "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
            "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (27ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar ...\n",
            "\t[SUCCESSFUL ] commons-cli#commons-cli;1.2!commons-cli.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar ...\n",
            "\t[SUCCESSFUL ] commons-lang#commons-lang;2.6!commons-lang.jar (33ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (30ms)\n",
            "downloading https://repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar ...\n",
            "\t[SUCCESSFUL ] jline#jline;2.14.6!jline.jar (27ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.20/commons-compress-1.20.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.20!commons-compress.jar (31ms)\n",
            "downloading https://repo1.maven.org/maven2/com/tdunning/json/1.8/json-1.8.jar ...\n",
            "\t[SUCCESSFUL ] com.tdunning#json;1.8!json.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.1/metrics-core-4.1.1.jar ...\n",
            "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.1!metrics-core.jar(bundle) (27ms)\n",
            "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-jvm/4.1.1/metrics-jvm-4.1.1.jar ...\n",
            "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-jvm;4.1.1!metrics-jvm.jar(bundle) (32ms)\n",
            "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-json/4.1.1/metrics-json-4.1.1.jar ...\n",
            "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-json;4.1.1!metrics-json.jar(bundle) (26ms)\n",
            "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.10.0/jackson-databind-2.10.0.jar ...\n",
            "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.10.0!jackson-databind.jar(bundle) (42ms)\n",
            "downloading https://repo1.maven.org/maven2/com/github/joshelser/dropwizard-metrics-hadoop-metrics2-reporter/0.1.2/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar ...\n",
            "\t[SUCCESSFUL ] com.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2!dropwizard-metrics-hadoop-metrics2-reporter.jar (26ms)\n",
            "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.10.0/jackson-annotations-2.10.0.jar ...\n",
            "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.10.0!jackson-annotations.jar(bundle) (26ms)\n",
            "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.10.0/jackson-core-2.10.0.jar ...\n",
            "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.10.0!jackson-core.jar(bundle) (31ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/2.7.2/hadoop-common-2.7.2.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-common;2.7.2!hadoop-common.jar (68ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/2.7.2/hadoop-annotations-2.7.2.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;2.7.2!hadoop-annotations.jar (27ms)\n",
            "downloading https://repo1.maven.org/maven2/com/google/guava/guava/14.0.1/guava-14.0.1.jar ...\n",
            "\t[SUCCESSFUL ] com.google.guava#guava;14.0.1!guava.jar(bundle) (52ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.4.1!commons-math3.jar (50ms)\n",
            "downloading https://repo1.maven.org/maven2/xmlenc/xmlenc/0.52/xmlenc-0.52.jar ...\n",
            "\t[SUCCESSFUL ] xmlenc#xmlenc;0.52!xmlenc.jar (26ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.5/commons-io-2.5.jar ...\n",
            "\t[SUCCESSFUL ] commons-io#commons-io;2.5!commons-io.jar (29ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-net/commons-net/3.1/commons-net-3.1.jar ...\n",
            "\t[SUCCESSFUL ] commons-net#commons-net;3.1!commons-net.jar (44ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar ...\n",
            "\t[SUCCESSFUL ] commons-collections#commons-collections;3.2.2!commons-collections.jar (36ms)\n",
            "downloading https://repo1.maven.org/maven2/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar ...\n",
            "\t[SUCCESSFUL ] javax.servlet#servlet-api;2.5!servlet-api.jar (30ms)\n",
            "downloading https://repo1.maven.org/maven2/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar ...\n",
            "\t[SUCCESSFUL ] org.mortbay.jetty#jetty;6.1.26!jetty.jar (36ms)\n",
            "downloading https://repo1.maven.org/maven2/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar ...\n",
            "\t[SUCCESSFUL ] org.mortbay.jetty#jetty-util;6.1.26!jetty-util.jar (30ms)\n",
            "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.14/jersey-core-1.14.jar ...\n",
            "\t[SUCCESSFUL ] com.sun.jersey#jersey-core;1.14!jersey-core.jar (36ms)\n",
            "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-json/1.14/jersey-json-1.14.jar ...\n",
            "\t[SUCCESSFUL ] com.sun.jersey#jersey-json;1.14!jersey-json.jar (29ms)\n",
            "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-server/1.14/jersey-server-1.14.jar ...\n",
            "\t[SUCCESSFUL ] com.sun.jersey#jersey-server;1.14!jersey-server.jar (37ms)\n",
            "downloading https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.0/jets3t-0.9.0.jar ...\n",
            "\t[SUCCESSFUL ] net.java.dev.jets3t#jets3t;0.9.0!jets3t.jar (39ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar ...\n",
            "\t[SUCCESSFUL ] commons-configuration#commons-configuration;1.6!commons-configuration.jar (38ms)\n",
            "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
            "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (30ms)\n",
            "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar ...\n",
            "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;2.5.0!protobuf-java.jar(bundle) (37ms)\n",
            "downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar ...\n",
            "\t[SUCCESSFUL ] com.google.code.gson#gson;2.2.4!gson.jar (30ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/2.7.2/hadoop-auth-2.7.2.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-auth;2.7.2!hadoop-auth.jar (27ms)\n",
            "downloading https://repo1.maven.org/maven2/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar ...\n",
            "\t[SUCCESSFUL ] com.jcraft#jsch;0.1.42!jsch.jar (29ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-client/2.13.0/curator-client-2.13.0.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.curator#curator-client;2.13.0!curator-client.jar(bundle) (72ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-recipes/2.13.0/curator-recipes-2.13.0.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.curator#curator-recipes;2.13.0!curator-recipes.jar(bundle) (30ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.htrace#htrace-core;3.1.0-incubating!htrace-core.jar (52ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
            "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (26ms)\n",
            "downloading https://repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar ...\n",
            "\t[SUCCESSFUL ] org.codehaus.jettison#jettison;1.1!jettison.jar(bundle) (30ms)\n",
            "downloading https://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar ...\n",
            "\t[SUCCESSFUL ] com.sun.xml.bind#jaxb-impl;2.2.3-1!jaxb-impl.jar (38ms)\n",
            "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar ...\n",
            "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-jaxrs;1.9.13!jackson-jaxrs.jar (27ms)\n",
            "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar ...\n",
            "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-xc;1.9.13!jackson-xc.jar (26ms)\n",
            "downloading https://repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar ...\n",
            "\t[SUCCESSFUL ] javax.xml.bind#jaxb-api;2.2.2!jaxb-api.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar ...\n",
            "\t[SUCCESSFUL ] javax.xml.stream#stax-api;1.0-2!stax-api.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/javax/activation/activation/1.1.1/activation-1.1.1.jar ...\n",
            "\t[SUCCESSFUL ] javax.activation#activation;1.1.1!activation.jar (26ms)\n",
            "downloading https://repo1.maven.org/maven2/asm/asm/3.2/asm-3.2.jar ...\n",
            "\t[SUCCESSFUL ] asm#asm;3.2!asm.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.12/httpcore-4.4.12.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.12!httpcore.jar (37ms)\n",
            "downloading https://repo1.maven.org/maven2/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlbuilder-0.4.jar ...\n",
            "\t[SUCCESSFUL ] com.jamesmurty.utils#java-xmlbuilder;0.4!java-xmlbuilder.jar (27ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-digester/commons-digester/1.8/commons-digester-1.8.jar ...\n",
            "\t[SUCCESSFUL ] commons-digester#commons-digester;1.8!commons-digester.jar (29ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar ...\n",
            "\t[SUCCESSFUL ] commons-beanutils#commons-beanutils-core;1.8.0!commons-beanutils-core.jar (32ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar ...\n",
            "\t[SUCCESSFUL ] commons-beanutils#commons-beanutils;1.9.4!commons-beanutils.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
            "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (26ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.2!snappy-java.jar(bundle) (80ms)\n",
            "downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.5/xz-1.5.jar ...\n",
            "\t[SUCCESSFUL ] org.tukaani#xz;1.5!xz.jar (26ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15!apacheds-kerberos-codec.jar(bundle) (35ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-framework/2.13.0/curator-framework-2.13.0.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.curator#curator-framework;2.13.0!curator-framework.jar(bundle) (30ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.directory.server#apacheds-i18n;2.0.0-M15!apacheds-i18n.jar(bundle) (29ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.directory.api#api-asn1-api;1.0.0-M20!api-asn1-api.jar(bundle) (35ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.directory.api#api-util;1.0.0-M20!api-util.jar(bundle) (28ms)\n",
            "downloading https://repo1.maven.org/maven2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar ...\n",
            "\t[SUCCESSFUL ] javax.servlet.jsp#jsp-api;2.1!jsp-api.jar (30ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hive/hive-vector-code-gen/2.3.7/hive-vector-code-gen-2.3.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hive#hive-vector-code-gen;2.3.7!hive-vector-code-gen.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
            "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.0.4/ST4-4.0.4.jar ...\n",
            "\t[SUCCESSFUL ] org.antlr#ST4;4.0.4!ST4.jar (120ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.ivy#ivy;2.4.0!ivy.jar (65ms)\n",
            "downloading https://repo1.maven.org/maven2/stax/stax-api/1.0.1/stax-api-1.0.1.jar ...\n",
            "\t[SUCCESSFUL ] stax#stax-api;1.0.1!stax-api.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/velocity/velocity/1.5/velocity-1.5.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.velocity#velocity;1.5!velocity.jar (23ms)\n",
            "downloading https://repo1.maven.org/maven2/oro/oro/2.0.8/oro-2.0.8.jar ...\n",
            "\t[SUCCESSFUL ] oro#oro;2.0.8!oro.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/javolution/javolution/5.5.1/javolution-5.5.1.jar ...\n",
            "\t[SUCCESSFUL ] javolution#javolution;5.5.1!javolution.jar(bundle) (40ms)\n",
            "downloading https://repo1.maven.org/maven2/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar ...\n",
            "\t[SUCCESSFUL ] com.jolbox#bonecp;0.8.0.RELEASE!bonecp.jar(bundle) (29ms)\n",
            "downloading https://repo1.maven.org/maven2/com/zaxxer/HikariCP/2.5.1/HikariCP-2.5.1.jar ...\n",
            "\t[SUCCESSFUL ] com.zaxxer#HikariCP;2.5.1!HikariCP.jar(bundle) (27ms)\n",
            "downloading https://repo1.maven.org/maven2/org/datanucleus/datanucleus-api-jdo/4.2.4/datanucleus-api-jdo-4.2.4.jar ...\n",
            "\t[SUCCESSFUL ] org.datanucleus#datanucleus-api-jdo;4.2.4!datanucleus-api-jdo.jar (40ms)\n",
            "downloading https://repo1.maven.org/maven2/org/datanucleus/datanucleus-rdbms/4.1.19/datanucleus-rdbms-4.1.19.jar ...\n",
            "\t[SUCCESSFUL ] org.datanucleus#datanucleus-rdbms;4.1.19!datanucleus-rdbms.jar (91ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar ...\n",
            "\t[SUCCESSFUL ] commons-pool#commons-pool;1.5.4!commons-pool.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar ...\n",
            "\t[SUCCESSFUL ] commons-dbcp#commons-dbcp;1.4!commons-dbcp.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar ...\n",
            "\t[SUCCESSFUL ] javax.jdo#jdo-api;3.0.1!jdo-api.jar (30ms)\n",
            "downloading https://repo1.maven.org/maven2/org/datanucleus/javax.jdo/3.2.0-m3/javax.jdo-3.2.0-m3.jar ...\n",
            "\t[SUCCESSFUL ] org.datanucleus#javax.jdo;3.2.0-m3!javax.jdo.jar (33ms)\n",
            "downloading https://repo1.maven.org/maven2/javax/transaction/jta/1.1/jta-1.1.jar ...\n",
            "\t[SUCCESSFUL ] javax.transaction#jta;1.1!jta.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/javax/transaction/transaction-api/1.1/transaction-api-1.1.jar ...\n",
            "\t[SUCCESSFUL ] javax.transaction#transaction-api;1.1!transaction-api.jar (26ms)\n",
            "downloading https://repo1.maven.org/maven2/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar ...\n",
            "\t[SUCCESSFUL ] net.sf.opencsv#opencsv;2.3!opencsv.jar (25ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hive/shims/hive-shims-common/2.3.7/hive-shims-common-2.3.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hive.shims#hive-shims-common;2.3.7!hive-shims-common.jar (34ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hive/shims/hive-shims-0.23/2.3.7/hive-shims-0.23-2.3.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hive.shims#hive-shims-0.23;2.3.7!hive-shims-0.23.jar (29ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hive/shims/hive-shims-scheduler/2.3.7/hive-shims-scheduler-2.3.7.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hive.shims#hive-shims-scheduler;2.3.7!hive-shims-scheduler.jar (28ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/avro/avro-ipc/1.8.2/avro-ipc-1.8.2.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.avro#avro-ipc;1.8.2!avro-ipc.jar(bundle) (30ms)\n",
            ":: resolution report :: resolve 21100ms :: artifacts dl 4150ms\n",
            "\t:: modules in use:\n",
            "\tasm#asm;3.2 from central in [default]\n",
            "\tcom.fasterxml.jackson.core#jackson-annotations;2.10.0 from central in [default]\n",
            "\tcom.fasterxml.jackson.core#jackson-core;2.10.0 from central in [default]\n",
            "\tcom.fasterxml.jackson.core#jackson-databind;2.10.0 from central in [default]\n",
            "\tcom.github.joshelser#dropwizard-metrics-hadoop-metrics2-reporter;0.1.2 from central in [default]\n",
            "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
            "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
            "\tcom.google.guava#guava;14.0.1 from central in [default]\n",
            "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
            "\tcom.jamesmurty.utils#java-xmlbuilder;0.4 from central in [default]\n",
            "\tcom.jcraft#jsch;0.1.42 from central in [default]\n",
            "\tcom.jolbox#bonecp;0.8.0.RELEASE from central in [default]\n",
            "\tcom.sun.jersey#jersey-core;1.14 from central in [default]\n",
            "\tcom.sun.jersey#jersey-json;1.14 from central in [default]\n",
            "\tcom.sun.jersey#jersey-server;1.14 from central in [default]\n",
            "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
            "\tcom.tdunning#json;1.8 from central in [default]\n",
            "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
            "\tcom.zaxxer#HikariCP;2.5.1 from central in [default]\n",
            "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
            "\tcommons-beanutils#commons-beanutils-core;1.8.0 from central in [default]\n",
            "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
            "\tcommons-codec#commons-codec;1.10 from central in [default]\n",
            "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
            "\tcommons-configuration#commons-configuration;1.6 from central in [default]\n",
            "\tcommons-dbcp#commons-dbcp;1.4 from central in [default]\n",
            "\tcommons-digester#commons-digester;1.8 from central in [default]\n",
            "\tcommons-httpclient#commons-httpclient;3.1 from central in [default]\n",
            "\tcommons-io#commons-io;2.5 from central in [default]\n",
            "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
            "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
            "\tcommons-net#commons-net;3.1 from central in [default]\n",
            "\tcommons-pool#commons-pool;1.5.4 from central in [default]\n",
            "\tio.dropwizard.metrics#metrics-core;4.1.1 from central in [default]\n",
            "\tio.dropwizard.metrics#metrics-json;4.1.1 from central in [default]\n",
            "\tio.dropwizard.metrics#metrics-jvm;4.1.1 from central in [default]\n",
            "\tjavax.activation#activation;1.1.1 from central in [default]\n",
            "\tjavax.jdo#jdo-api;3.0.1 from central in [default]\n",
            "\tjavax.servlet#servlet-api;2.5 from central in [default]\n",
            "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
            "\tjavax.transaction#jta;1.1 from central in [default]\n",
            "\tjavax.transaction#transaction-api;1.1 from central in [default]\n",
            "\tjavax.xml.bind#jaxb-api;2.2.2 from central in [default]\n",
            "\tjavax.xml.stream#stax-api;1.0-2 from central in [default]\n",
            "\tjavolution#javolution;5.5.1 from central in [default]\n",
            "\tjline#jline;2.14.6 from central in [default]\n",
            "\tjoda-time#joda-time;2.10.5 from central in [default]\n",
            "\tnet.java.dev.jets3t#jets3t;0.9.0 from central in [default]\n",
            "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
            "\torg.antlr#ST4;4.0.4 from central in [default]\n",
            "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
            "\torg.apache.avro#avro;1.8.2 from central in [default]\n",
            "\torg.apache.avro#avro-ipc;1.8.2 from central in [default]\n",
            "\torg.apache.avro#avro-mapred;1.8.2 from central in [default]\n",
            "\torg.apache.commons#commons-compress;1.20 from central in [default]\n",
            "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
            "\torg.apache.commons#commons-math3;3.4.1 from central in [default]\n",
            "\torg.apache.curator#curator-client;2.13.0 from central in [default]\n",
            "\torg.apache.curator#curator-framework;2.13.0 from central in [default]\n",
            "\torg.apache.curator#curator-recipes;2.13.0 from central in [default]\n",
            "\torg.apache.derby#derby;10.12.1.1 from central in [default]\n",
            "\torg.apache.directory.api#api-asn1-api;1.0.0-M20 from central in [default]\n",
            "\torg.apache.directory.api#api-util;1.0.0-M20 from central in [default]\n",
            "\torg.apache.directory.server#apacheds-i18n;2.0.0-M15 from central in [default]\n",
            "\torg.apache.directory.server#apacheds-kerberos-codec;2.0.0-M15 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-annotations;2.7.2 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-auth;2.7.2 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-common;2.7.2 from central in [default]\n",
            "\torg.apache.hive#hive-common;2.3.7 from central in [default]\n",
            "\torg.apache.hive#hive-exec;2.3.7 from central in [default]\n",
            "\torg.apache.hive#hive-llap-client;2.3.7 from central in [default]\n",
            "\torg.apache.hive#hive-llap-common;2.3.7 from central in [default]\n",
            "\torg.apache.hive#hive-metastore;2.3.7 from central in [default]\n",
            "\torg.apache.hive#hive-serde;2.3.7 from central in [default]\n",
            "\torg.apache.hive#hive-shims;2.3.7 from central in [default]\n",
            "\torg.apache.hive#hive-vector-code-gen;2.3.7 from central in [default]\n",
            "\torg.apache.hive.shims#hive-shims-0.23;2.3.7 from central in [default]\n",
            "\torg.apache.hive.shims#hive-shims-common;2.3.7 from central in [default]\n",
            "\torg.apache.hive.shims#hive-shims-scheduler;2.3.7 from central in [default]\n",
            "\torg.apache.htrace#htrace-core;3.1.0-incubating from central in [default]\n",
            "\torg.apache.httpcomponents#httpclient;4.5.6 from central in [default]\n",
            "\torg.apache.httpcomponents#httpcore;4.4.12 from central in [default]\n",
            "\torg.apache.ivy#ivy;2.4.0 from central in [default]\n",
            "\torg.apache.spark#spark-hive_2.12;3.1.1 from central in [default]\n",
            "\torg.apache.thrift#libfb303;0.9.3 from central in [default]\n",
            "\torg.apache.thrift#libthrift;0.12.0 from central in [default]\n",
            "\torg.apache.velocity#velocity;1.5 from central in [default]\n",
            "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
            "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
            "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
            "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
            "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
            "\torg.datanucleus#datanucleus-api-jdo;4.2.4 from central in [default]\n",
            "\torg.datanucleus#datanucleus-core;4.1.17 from central in [default]\n",
            "\torg.datanucleus#datanucleus-rdbms;4.1.19 from central in [default]\n",
            "\torg.datanucleus#javax.jdo;3.2.0-m3 from central in [default]\n",
            "\torg.jodd#jodd-core;3.5.2 from central in [default]\n",
            "\torg.mortbay.jetty#jetty;6.1.26 from central in [default]\n",
            "\torg.mortbay.jetty#jetty-util;6.1.26 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.tukaani#xz;1.5 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
            "\toro#oro;2.0.8 from central in [default]\n",
            "\tstax#stax-api;1.0.1 from central in [default]\n",
            "\txmlenc#xmlenc;0.52 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |  106  |  106  |  106  |   0   ||  106  |  106  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-74c8f42c-58b1-42d5-a00b-293124ec28e8\n",
            "\tconfs: [default]\n",
            "\t106 artifacts copied, 0 already retrieved (63355kB/469ms)\n",
            "25/03/07 09:50:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/03/07 09:50:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "Welcome to\n",
            "      ____              __\n",
            "     / __/__  ___ _____/ /__\n",
            "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
            "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n",
            "      /_/\n",
            "\n",
            "Using Python version 3.11.11 (main, Dec  4 2024 08:55:07)\n",
            "Spark context Web UI available at http://817b9707dfe5:4041\n",
            "Spark context available as 'sc' (master = local[*], app id = local-1741341036170).\n",
            "SparkSession available as 'spark'.\n",
            ">>> spark.sql(\"\"\"  CREATE TABLE flights_table (   date STRING,  delay INT,  distance INT,  origin STRING,  destination STRING  ) \"\"\") \n",
            "25/03/07 09:53:58 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
            "25/03/07 09:53:59 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
            "25/03/07 09:53:59 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
            "25/03/07 09:54:00 WARN General: Plugin (Bundle) \"org.datanucleus\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/root/.ivy2/jars/org.datanucleus_datanucleus-core-4.1.17.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/datanucleus-core-4.1.17.jar.\"\n",
            "25/03/07 09:54:00 WARN General: Plugin (Bundle) \"org.datanucleus.api.jdo\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/root/.ivy2/jars/org.datanucleus_datanucleus-api-jdo-4.2.4.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/datanucleus-api-jdo-4.2.4.jar.\"\n",
            "25/03/07 09:54:00 WARN General: Plugin (Bundle) \"org.datanucleus.store.rdbms\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/root/.ivy2/jars/org.datanucleus_datanucleus-rdbms-4.1.19.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/datanucleus-rdbms-4.1.19.jar.\"\n",
            "25/03/07 09:54:09 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
            "25/03/07 09:54:09 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.28.0.12\n",
            "25/03/07 09:54:09 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
            "25/03/07 09:54:09 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
            "25/03/07 09:54:10 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
            "25/03/07 09:54:10 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
            "25/03/07 09:54:10 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
            "25/03/07 09:54:10 WARN HiveMetaStore: Location: file:/content/spark-warehouse/flights_table specified for non-external table:flights_table\n",
            "DataFrame[]\n",
            ">>> spark.sql(\"\"\" CREATE TABLE flights_table USING hive AS SELECT * FROM some_other_table \"\"\")\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 1, in <module>\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\", line 1631, in sql\n",
            "    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
            "    raise converted from None\n",
            "pyspark.errors.exceptions.captured.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `some_other_table` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
            "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
            "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 56;\n",
            "'CreateTable `spark_catalog`.`default`.`flights_table`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists\n",
            "+- 'Project [*]\n",
            "   +- 'UnresolvedRelation [some_other_table], [], false\n",
            "\n",
            ">>> exit\n",
            "Use exit() or Ctrl-D (i.e. EOF) to exit\n",
            ">>> exit()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceGlobalTempView(\"global_flight_view\")\n",
        "# Querying the global temporary view\n",
        "spark.sql(\"SELECT * FROM global_temp.global_flight_view\").show()"
      ],
      "metadata": {
        "id": "sqd-2o-hBRMi",
        "outputId": "f6dcd14a-1db3-4ed3-e5a3-7e5cdaa1dfee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sqd-2o-hBRMi",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------+------+-----------+\n",
            "|   date|delay|distance|origin|destination|\n",
            "+-------+-----+--------+------+-----------+\n",
            "|1011245|    6|     602|   ABE|        ATL|\n",
            "|1020600|   -8|     369|   ABE|        DTW|\n",
            "|1021245|   -2|     602|   ABE|        ATL|\n",
            "|1020605|   -4|     602|   ABE|        ATL|\n",
            "|1031245|   -4|     602|   ABE|        ATL|\n",
            "|1030605|    0|     602|   ABE|        ATL|\n",
            "|1041243|   10|     602|   ABE|        ATL|\n",
            "|1040605|   28|     602|   ABE|        ATL|\n",
            "|1051245|   88|     602|   ABE|        ATL|\n",
            "|1050605|    9|     602|   ABE|        ATL|\n",
            "|1061215|   -6|     602|   ABE|        ATL|\n",
            "|1061725|   69|     602|   ABE|        ATL|\n",
            "|1061230|    0|     369|   ABE|        DTW|\n",
            "|1060625|   -3|     602|   ABE|        ATL|\n",
            "|1070600|    0|     369|   ABE|        DTW|\n",
            "|1071725|    0|     602|   ABE|        ATL|\n",
            "|1071230|    0|     369|   ABE|        DTW|\n",
            "|1070625|    0|     602|   ABE|        ATL|\n",
            "|1071219|    0|     569|   ABE|        ORD|\n",
            "|1080600|    0|     369|   ABE|        DTW|\n",
            "+-------+-----+--------+------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' contains your flight data\n",
        "df.createOrReplaceTempView(\"flights_table\")\n",
        "\n",
        "# Now you can query it:\n",
        "flights_df = spark.sql(\"SELECT * FROM flights_table\")\n",
        "flights_df.show()"
      ],
      "metadata": {
        "id": "doUNtvY6CXjC",
        "outputId": "27198bdd-abb4-41bd-9a47-0428c88d61d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "doUNtvY6CXjC",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------+------+-----------+\n",
            "|   date|delay|distance|origin|destination|\n",
            "+-------+-----+--------+------+-----------+\n",
            "|1011245|    6|     602|   ABE|        ATL|\n",
            "|1020600|   -8|     369|   ABE|        DTW|\n",
            "|1021245|   -2|     602|   ABE|        ATL|\n",
            "|1020605|   -4|     602|   ABE|        ATL|\n",
            "|1031245|   -4|     602|   ABE|        ATL|\n",
            "|1030605|    0|     602|   ABE|        ATL|\n",
            "|1041243|   10|     602|   ABE|        ATL|\n",
            "|1040605|   28|     602|   ABE|        ATL|\n",
            "|1051245|   88|     602|   ABE|        ATL|\n",
            "|1050605|    9|     602|   ABE|        ATL|\n",
            "|1061215|   -6|     602|   ABE|        ATL|\n",
            "|1061725|   69|     602|   ABE|        ATL|\n",
            "|1061230|    0|     369|   ABE|        DTW|\n",
            "|1060625|   -3|     602|   ABE|        ATL|\n",
            "|1070600|    0|     369|   ABE|        DTW|\n",
            "|1071725|    0|     602|   ABE|        ATL|\n",
            "|1071230|    0|     369|   ABE|        DTW|\n",
            "|1070625|    0|     602|   ABE|        ATL|\n",
            "|1071219|    0|     569|   ABE|        ORD|\n",
            "|1080600|    0|     369|   ABE|        DTW|\n",
            "+-------+-----+--------+------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.cacheTable(\"flights_table\")"
      ],
      "metadata": {
        "id": "vAUVJCfVCeXh"
      },
      "id": "vAUVJCfVCeXh",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.saveAsTable(\"flights_table\")"
      ],
      "metadata": {
        "id": "CEqikaumDF2w"
      },
      "id": "CEqikaumDF2w",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"DROP TABLE IF EXISTS flights_table\")"
      ],
      "metadata": {
        "id": "fiPR5xMTDIdv",
        "outputId": "8f64f835-0c61-4fbc-fb43-c4705ccbf137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fiPR5xMTDIdv",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.partitionBy(\"origin\",\n",
        "\"destination\").format(\"csv\").save(\"/path/to/output\")"
      ],
      "metadata": {
        "id": "ENacL9rGDl9P"
      },
      "id": "ENacL9rGDl9P",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filled = df.fillna({\"delay\": 0})\n",
        "df_filled.show()"
      ],
      "metadata": {
        "id": "X0LT-pIiEVZv",
        "outputId": "b726d4a7-6463-42d3-b4fb-0824ac1fd55d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "X0LT-pIiEVZv",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+--------+------+-----------+\n",
            "|   date|delay|distance|origin|destination|\n",
            "+-------+-----+--------+------+-----------+\n",
            "|1011245|    6|     602|   ABE|        ATL|\n",
            "|1020600|   -8|     369|   ABE|        DTW|\n",
            "|1021245|   -2|     602|   ABE|        ATL|\n",
            "|1020605|   -4|     602|   ABE|        ATL|\n",
            "|1031245|   -4|     602|   ABE|        ATL|\n",
            "|1030605|    0|     602|   ABE|        ATL|\n",
            "|1041243|   10|     602|   ABE|        ATL|\n",
            "|1040605|   28|     602|   ABE|        ATL|\n",
            "|1051245|   88|     602|   ABE|        ATL|\n",
            "|1050605|    9|     602|   ABE|        ATL|\n",
            "|1061215|   -6|     602|   ABE|        ATL|\n",
            "|1061725|   69|     602|   ABE|        ATL|\n",
            "|1061230|    0|     369|   ABE|        DTW|\n",
            "|1060625|   -3|     602|   ABE|        ATL|\n",
            "|1070600|    0|     369|   ABE|        DTW|\n",
            "|1071725|    0|     602|   ABE|        ATL|\n",
            "|1071230|    0|     369|   ABE|        DTW|\n",
            "|1070625|    0|     602|   ABE|        ATL|\n",
            "|1071219|    0|     569|   ABE|        ORD|\n",
            "|1080600|    0|     369|   ABE|        DTW|\n",
            "+-------+-----+--------+------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8sG0iok1Ew9u"
      },
      "id": "8sG0iok1Ew9u",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}